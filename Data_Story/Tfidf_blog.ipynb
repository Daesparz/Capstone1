{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Part 1\n",
    "### TF-IDF score for blogs\n",
    "After scrapping a blog website, we could extract key phares/words to rank and classify the kind of information that experts put on blogs. In this stage of the project, we label data manually. How we do that considering that every website has thousand of words? \n",
    "* Extract hopeful phrases\n",
    "Bag of words is one of the most popular useful techniques for handling meaningful features, but in large corporal, is not the appropiate probably. Barg of words works with absolute term frequency, so all words that you don't delete as stopwords and have high frecuency could overshadow other more important terms. TF-IDF model use a normalized factor that we are using in the following develop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building **expandContractions** function and definition of stop words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "all credits go to alko and arturomp @ stack overflow.\n",
    "\"\"\"\n",
    "\n",
    "with open('../Data_Story/wordLists/contractionList.txt', 'r') as f:\n",
    "    cList = json.loads(f.read())\n",
    "    c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from Data Extraction folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data_Extraction/reviews_blog_a.csv', usecols=['Name', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stemmer = PorterStemmer() \n",
    "stop_words = []\n",
    "with open('../Data_Story/wordLists/stop_wordsList.txt') as f:\n",
    "    stop_words = f.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining **normalizer** function to expand contractions, remove special characters and split all sentences. We use tokenizer to split sentences in words and delete stop words. Then, words and sentences are joining again. The result is a **corpus normalized**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(paragrahp):\n",
    "    paragrahp = expandContractions(paragrahp.lower())\n",
    "    paragrahp_c = paragrahp.split('.')[:-1]\n",
    "    # lower case and remove special character/whitespace\n",
    "    paragrahp = [re.sub(r'[^a-zA-Z\\s]','', sentence) for sentence in paragrahp_c]\n",
    "    paragrahp = [sentence.strip() for sentence in paragrahp]\n",
    "    tokens = [wpt.tokenize(sentence) for sentence in paragrahp]\n",
    "    tokens_filtered = [[word for word in token if word not in stop_words] for token in tokens]\n",
    "    paragrahp_norm = [' '.join(tokens) for tokens in tokens_filtered]\n",
    "    return paragrahp_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining **key word extractor** function that computes a normalized frequency of terms in pairs. We filter lower scores to build a dataframe with a **vocabulary of phrases and TF-IDF values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_word_extractor(norm_corpus):\n",
    "    tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True, ngram_range=(2,2), max_features=40)\n",
    "    tv_matrix = tv.fit_transform(norm_corpus)\n",
    "    tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "    idx = [i for i in range(len(norm_corpus))]\n",
    "    vocab = tv.get_feature_names()\n",
    "    df_tfidV = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab, index=idx)\n",
    "\n",
    "    key_values = []\n",
    "    key_words = []\n",
    "    for i in range(len(norm_corpus)):\n",
    "        for column in vocab:\n",
    "            if df_tfidV[column][i] > 0.20:\n",
    "                key_values.append(df_tfidV[column][i])\n",
    "                key_words.append(column)\n",
    "    return key_words, key_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the below functions to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [key_word_extractor(normalizer(review))[0] for review in df['Description']]   \n",
    "key_values = [key_word_extractor(normalizer(review))[1] for review in df['Description']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words = []\n",
    "l_values = []\n",
    "for list_words in key_words:\n",
    "    l_words = [*l_words, *list_words] \n",
    "    \n",
    "for list_values in key_values:\n",
    "    l_values = [*l_values, *list_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'key_words': l_words, 'key_values': l_values}\n",
    "df_keys = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to examine the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_words</th>\n",
       "      <th>key_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toast crave</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cinnamon sugar</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mitigate toast</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nutella mitigate</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slathered nutella</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sugar slathered</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toast cravings</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>best coffee</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>coffee neighborhood</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>best parklet</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>composed huge</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expect find</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>find seat</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>head best</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huge twisted</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inside not</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>instead head</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>not expect</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>parklet town</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>seat instead</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              key_words  key_values\n",
       "0           toast crave        1.00\n",
       "1        cinnamon sugar        0.41\n",
       "2        mitigate toast        0.41\n",
       "3      nutella mitigate        0.41\n",
       "4     slathered nutella        0.41\n",
       "5       sugar slathered        0.41\n",
       "6        toast cravings        0.41\n",
       "7           best coffee        0.71\n",
       "8   coffee neighborhood        0.71\n",
       "9          best parklet        0.26\n",
       "10        composed huge        0.26\n",
       "11          expect find        0.26\n",
       "12            find seat        0.26\n",
       "13            head best        0.26\n",
       "14         huge twisted        0.26\n",
       "15           inside not        0.26\n",
       "16         instead head        0.26\n",
       "17           not expect        0.26\n",
       "18         parklet town        0.26\n",
       "19         seat instead        0.26"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keys.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save dataframe as csv file for manual labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keys.to_csv('preprocessing/key_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_words has a round of one thousand of words. We define a **category** and a **section** for each one. Criteria is the following:\n",
    "\n",
    "1. Only are considered the phrases with a concrete and valid meaning\n",
    "2. If a phrase classify as meaningful, it belongs to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Coffee**: all phrases relatives to types of drinks, beans, roasters, baristas, special types of sugar, milk,  and items that involving the experience about the cup of coffee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) **Place**: place features as decorationg, description of inside and outside spaces, parklets, sunsets from the seat, music, gardens, streets around; about the service itself (coffee to here, to go, wifi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) **Food**: phrases about pastries, donuts, bagels, baked items in general, sandwiches, phrases relatives to breakfast and lunch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) **None**: all the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Each phrase gets a category and a section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coffee: Baristas, Roasting, Beans, Drinks, Sentiment, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Place: Decoration, To here, To go, Outside, Sentiment, Size of the coffee shop, wifi, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Food: Sentiment, Breakfast, Baked (tiny items to eat with the coffee), Lunch and Brunch, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

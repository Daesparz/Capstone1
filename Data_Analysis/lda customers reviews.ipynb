{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8e/ae1b656131601fb0f31795f4b0b2a3beb5450fba63a2c87144f0f18807bc/gensim-3.7.3-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 952kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.18.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/70/9f8f1e9ed9fd33b96bb27fb080171fb571f7d33fecbd857e9f504e6dcc9a/scipy-1.2.1-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (27.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 27.4MB 881kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.11.3 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/0b/1a2c21bb69138337dc079841aa4a45e5b2fc7a4260c0907f5254fb08f02e/numpy-1.16.4-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 5.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python2.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting boto>=2.32 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/5e/d109e3ec9a6b43887ed9de1bd16141ad8e4442aaecff6f90fca05429c43a/boto3-1.9.159-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 18.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 15.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl (158kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 17.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 18.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 23.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.159 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/bb/113f5083f145c41750d5b6f171a76c20f2e2b55a7bd9d02f55e849559619/botocore-1.12.159-py2.py3-none-any.whl (5.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 7.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.13.0,>=1.12.159->boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.159->boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/c53398e0005b11f7ffb27b7aa720c617aba53be4fb4f4f3f06b9b5c60f28/docutils-0.14-py2-none-any.whl (543kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 17.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" (from s3transfer<0.3.0,>=0.2.0->boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/daniela/Library/Caches/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/daniela/Library/Caches/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: numpy, scipy, boto, urllib3, certifi, chardet, idna, requests, jmespath, python-dateutil, docutils, botocore, futures, s3transfer, boto3, bz2file, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.9.159 botocore-1.12.159 bz2file-0.98 certifi-2019.3.9 chardet-3.0.4 docutils-0.14 futures-3.2.0 gensim-3.7.3 idna-2.8 jmespath-0.9.4 numpy-1.16.4 python-dateutil-2.8.0 requests-2.22.0 s3transfer-0.2.0 scipy-1.2.1 smart-open-1.8.4 urllib3-1.25.3\n"
     ]
    }
   ],
   "source": [
    " #!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "all credits go to alko and arturomp @ stack overflow.\n",
    "\"\"\"\n",
    "\n",
    "f = open('../Data_Story/wordLists/contractionList.txt', 'r')\n",
    "cList = json.loads(f.read())\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#stop_words = ['do','got','did','made', 'make','also','been','like','is', 'was', 'were', \\\n",
    "#               'are', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \\\n",
    "#               \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \\\n",
    "#               'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \\\n",
    "#               \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', \\\n",
    "#               'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'a', \\\n",
    "#               'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "#               'at', 'by', 'for', 'with', 'about', 'against', 'between', 'through', 'during', \\\n",
    "#               'before', 'after', 'above', 'below', 'to', 'from', 'in', 'out', 'on', 'off', \\\n",
    "#               'under', 'again', 'further', 'then', 'once', 'here', 'there', 'why', 'how', \\\n",
    "#               'all', 'any', 'both', 'each', 'other', 'some', 'such', 'same', 'so', 'than', \\\n",
    "#               'too', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \\\n",
    "#               'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \\\n",
    "#               \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \\\n",
    "#               \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", \\\n",
    "#               'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \\\n",
    "#               'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'yeah', \\\n",
    "#               'yea', 'san', 'fran', 'francisco']\n",
    "\n",
    "new_words=('yeah', 'yea', 'also', 'get', 'got', 'etc','theyvenconcocted', 'ive', 'san', 'fran', \\\n",
    "           'francisco', 'sf', 'finally', 'wasnt', 'cool', 'end', 'im' , 'yelp', 'could', 'would', \\\n",
    "           'cannot', 'wouldnt','way', 'day', 'yum', )\n",
    "for i in new_words:\n",
    "    stop_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "    \n",
    "def normalizer(paragrahp):\n",
    "    paragrahp = expandContractions(paragrahp.lower())\n",
    "    paragrahp_c = paragrahp.split('.')[:-1]\n",
    "    # lower case and remove special character/whitespace\n",
    "    paragrahp = [re.sub(r'[^a-zA-Z\\s]','', sentence) for sentence in paragrahp_c]\n",
    "    #paragrahp = [sentence.strip() for sentence in paragrahp]\n",
    "    tokens = [wpt.tokenize(sentence) for sentence in paragrahp]\n",
    "    tokens_filtered = [[word for word in token if word not in stop_words] for token in tokens]\n",
    "    tokens_lemmatizer = [[lemmatizer.lemmatize(word) for word in token] for token in tokens_filtered]\n",
    "    #paragrahp_norm = [' '.join(tokens) for tokens in tokens_filtered]\n",
    "    return tokens_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data_Extraction/Reviews/reviews_yelp_scrapping_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Native Twins Coffee</td>\n",
       "      <td>[\"This place to caters to all types of eaters....</td>\n",
       "      <td>['5.0 star rating', '3.0 star rating', '5.0 st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home</td>\n",
       "      <td>[\"Another cafe that has been on my bookmark in...</td>\n",
       "      <td>['5.0 star rating', '3.0 star rating', '5.0 st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home</td>\n",
       "      <td>[\"Visiting friends in SF, my husband was cravi...</td>\n",
       "      <td>['5.0 star rating', '4.0 star rating', '4.0 st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Réveille Coffee Co.</td>\n",
       "      <td>[\"Love the vibe in this cozy little space! It'...</td>\n",
       "      <td>['5.0 star rating', '1.0 star rating', '4.0 st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neighbor’s Corner</td>\n",
       "      <td>[\"This charming cafe served up much more than ...</td>\n",
       "      <td>['5.0 star rating', '5.0 star rating', '5.0 st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name                                        Description  \\\n",
       "0  Native Twins Coffee  [\"This place to caters to all types of eaters....   \n",
       "1                 Home  [\"Another cafe that has been on my bookmark in...   \n",
       "2                 Home  [\"Visiting friends in SF, my husband was cravi...   \n",
       "3  Réveille Coffee Co.  [\"Love the vibe in this cozy little space! It'...   \n",
       "4    Neighbor’s Corner  [\"This charming cafe served up much more than ...   \n",
       "\n",
       "                                              Rating  \n",
       "0  ['5.0 star rating', '3.0 star rating', '5.0 st...  \n",
       "1  ['5.0 star rating', '3.0 star rating', '5.0 st...  \n",
       "2  ['5.0 star rating', '4.0 star rating', '4.0 st...  \n",
       "3  ['5.0 star rating', '1.0 star rating', '4.0 st...  \n",
       "4  ['5.0 star rating', '5.0 star rating', '5.0 st...  "
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def lda_coffeeshop(id, topics=1, num_words=1):\n",
    "    data = df['Description'][id]\n",
    "    texts = normalizer(data)\n",
    "    #print (texts)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topics, id2word = dictionary, passes=50, random_state=10)\n",
    "    a = ldamodel.print_topics(num_topics=topics, num_words=num_words)\n",
    "    print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.697483336300947\n",
      "\n",
      "Coherence Score:  0.45592384126980434\n",
      "\n",
      "Perplexity:  -6.138733846082285\n",
      "\n",
      "Coherence Score:  0.44902192790552836\n"
     ]
    }
   ],
   "source": [
    "lda = lda_coffeeshop(0, 6, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda1 = lda_coffeeshop(0, 1, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"coffee\" + 0.013*\"latte\" + 0.012*\"place\" + 0.012*\"toast\" + 0.010*\"good\" + 0.010*\"great\" + 0.010*\"twin\" + 0.009*\"shop\" + 0.009*\"milk\" + 0.006*\"perfect\" + 0.006*\"really\" + 0.006*\"always\" + 0.006*\"delicious\" + 0.006*\"avocado\" + 0.006*\"matcha\" + 0.006*\"spot\" + 0.005*\"back\" + 0.005*\"native\" + 0.005*\"love\" + 0.005*\"super\" + 0.005*\"like\" + 0.004*\"nice\" + 0.004*\"though\" + 0.004*\"one\" + 0.004*\"turmeric\" + 0.004*\"bit\" + 0.004*\"cute\" + 0.004*\"favorite\" + 0.004*\"owned\" + 0.004*\"plus\" + 0.004*\"stopped\" + 0.004*\"cold\" + 0.004*\"friendly\" + 0.004*\"pastry\" + 0.004*\"brew\" + 0.003*\"definitely\" + 0.003*\"local\" + 0.003*\"bread\" + 0.003*\"service\" + 0.003*\"small\" + 0.003*\"drink\" + 0.003*\"business\" + 0.003*\"coming\" + 0.003*\"side\" + 0.003*\"tried\" + 0.003*\"chai\" + 0.003*\"made\" + 0.003*\"haight\" + 0.003*\"discovered\" + 0.003*\"enjoyed\" + 0.003*\"kind\" + 0.003*\"morning\" + 0.003*\"sister\" + 0.003*\"lovely\" + 0.003*\"pretty\" + 0.003*\"amazing\" + 0.003*\"start\" + 0.003*\"neighborhood\" + 0.003*\"everything\" + 0.003*\"time\" + 0.003*\"find\" + 0.003*\"bean\" + 0.003*\"almond\" + 0.003*\"recommend\" + 0.003*\"food\" + 0.003*\"little\" + 0.002*\"option\" + 0.002*\"beyond\" + 0.002*\"couple\" + 0.002*\"sidewalk\" + 0.002*\"many\" + 0.002*\"never\" + 0.002*\"ordered\" + 0.002*\"oak\" + 0.002*\"enjoy\" + 0.002*\"home\" + 0.002*\"thing\" + 0.002*\"staff\" + 0.002*\"whole\" + 0.002*\"miss\" + 0.002*\"barista\" + 0.002*\"garlic\" + 0.002*\"several\" + 0.002*\"butter\" + 0.002*\"tasty\" + 0.002*\"comfortable\" + 0.002*\"support\" + 0.002*\"woman\" + 0.002*\"superb\" + 0.002*\"outside\" + 0.002*\"come\" + 0.002*\"without\" + 0.002*\"even\" + 0.002*\"used\" + 0.002*\"lady\" + 0.002*\"order\" + 0.002*\"big\" + 0.002*\"lot\" + 0.002*\"pesto\" + 0.002*\"divisadero\"')]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['Description'][1]\n",
    "paragrahp = expandContractions(data).lower().split('.')\n",
    "paragrahp = [re.sub(r'[^a-zA-Z\\s]','', sentence) for sentence in paragrahp]\n",
    "tokens = [wpt.tokenize(sentence) for sentence in paragrahp]\n",
    "tokens_lemmatizer = [[lemmatizer.lemmatize(word) for word in token] for token in tokens]\n",
    "tokens_filtered = [[word for word in sentence if word not in stop_words] for sentence in tokens_lemmatizer]\n",
    "#print (tokens_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['coffee', 'toast', 'avocado', 'bean', 'barista', 'neghborhood', \\\n",
    "          'tiny', 'small', 'wifi', 'tasty', 'little', 'good', 'perfect', \\\n",
    "         'latte', 'capuccino', 'matcha', 'coming', 'cafe', 'pretty', 'almond', \\\n",
    "          'amazing']]\n",
    "dictionary = corpora.Dictionary(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.315*\"pretty\" + 0.264*\"little\" + 0.213*\"cafe\" + 0.060*\"wifi\" + 0.010*\"good\"'),\n",
       " (1,\n",
       "  '0.432*\"latte\" + 0.267*\"toast\" + 0.179*\"good\" + 0.026*\"avocado\" + 0.025*\"tasty\"'),\n",
       " (2,\n",
       "  '0.552*\"coffee\" + 0.123*\"perfect\" + 0.094*\"coming\" + 0.066*\"small\" + 0.011*\"latte\"')]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tokens_filtered]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=1000)\n",
    "ldamodel.print_topics(num_topics=3, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rating = df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [j.split('.')[0][-1] for ratings in list_rating for j in ratings.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df['Rating'].str.split(' star rating').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dictionary\n",
    "\n",
    "The dictionary was built using the three blogs cited previously, and the 5% of customers reviews group by rating score. We noted that we lose valuable information using only the keywords generated by blogs. Thus, we choose randomly the 5% of reviews with score 1, 5% with score 2 and so on, for trying to rescue a text representative sample of what people are talking about in every categorical score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions for pre-processing stage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building **expandContractions** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "all credits go to alko and arturomp @ stack overflow.\n",
    "\"\"\"\n",
    "\n",
    "with open('../Data_Story/wordLists/contractionList.txt', 'r') as f:\n",
    "    cList = json.loads(f.read())\n",
    "    c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "Add name of coffee shops as stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('../Data_Story/wordLists/stop_wordsList.txt') as f:\n",
    "    stop_words = f.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline preprocessing\n",
    "\n",
    "1. Normalize and expand contractions\n",
    "2. Delete spetial characters\n",
    "3. Tokenize words for filtering stopwords\n",
    "4. Lemmatization\n",
    "5. Join sentences again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def pre_processing(text):\n",
    "    # Normalize constractions and apply expansion of Constractions\n",
    "    text = re.sub(r'’',\"'\", text)\n",
    "    text = expandContractions(text.lower())\n",
    "    # Filtering special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]','', text)\n",
    "    # Tokenization and filtering stop-words\n",
    "    tokens = wpt.tokenize(text)\n",
    "    words = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    words_lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text_norm = ' '.join(words_lem)\n",
    "    \n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building TF-IDF function\n",
    "\n",
    "1. Build a Vectorizer model of TF-IDF with pairs of words (bi-grams)\n",
    "2. Fit the model with the input of the function (a normalized text)\n",
    "3. Transform matrix TD-IDF to an array\n",
    "4. Use a threshold to filter some TF-IDF values\n",
    "5. Return normalized freq and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def key_word_extractor(norm_corpus):\n",
    "    tv = TfidfVectorizer(use_idf=True, ngram_range=(2,2))\n",
    "    tv_matrix = tv.fit_transform(norm_corpus)\n",
    "    tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "    idx = [i for i in range(len(norm_corpus))]\n",
    "    vocab = tv.get_feature_names()\n",
    "    df_tfidV = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab, index=idx)\n",
    "\n",
    "    key_values = []\n",
    "    key_words = []\n",
    "    for i in range(len(norm_corpus)):\n",
    "        for column in vocab:\n",
    "            if df_tfidV[column][i] > 0.0001:\n",
    "                key_values.append(df_tfidV[column][i])\n",
    "                key_words.append(column)\n",
    "    return key_words, key_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blogs\n",
    "\n",
    "#### Preparing Blog reviews for analysis\n",
    "Merging all blogs in one dataset. A column of Blog is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data_Extraction/Reviews/reviews_blog_a.csv', usecols=['Name', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Review</th>\n",
       "      <th>Blog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trouble Coffee</td>\n",
       "      <td>Yeah yeah, they've got the toast you crave. Ci...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andytown Coffee Roasters</td>\n",
       "      <td>This small Outer Sunset shop chooses and roast...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Garden House Cafe</td>\n",
       "      <td>In the reaches of the Outer Richmond lies Gard...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snowbird Coffee</td>\n",
       "      <td>Opened by former filmmaker Eugene Kim, Snowbir...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flywheel Coffee</td>\n",
       "      <td>Haight-Ashbury's new school roaster and cafe F...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Names  \\\n",
       "0            Trouble Coffee   \n",
       "1  Andytown Coffee Roasters   \n",
       "2         Garden House Cafe   \n",
       "3           Snowbird Coffee   \n",
       "4           Flywheel Coffee   \n",
       "\n",
       "                                              Review Blog  \n",
       "0  Yeah yeah, they've got the toast you crave. Ci...    A  \n",
       "1  This small Outer Sunset shop chooses and roast...    A  \n",
       "2  In the reaches of the Outer Richmond lies Gard...    A  \n",
       "3  Opened by former filmmaker Eugene Kim, Snowbir...    A  \n",
       "4  Haight-Ashbury's new school roaster and cafe F...    A  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Blog'] = 'A'\n",
    "df = df.rename(index=str, columns={\"Name\": \"Names\", \"Description\": \"Review\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../Data_Extraction/Reviews/reviews_blogs_bc.csv', usecols=['Names', 'Wifi', 'Review', 'Blog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blogs = df1.append(df, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying preprocessing to blogs. We built the list **docs** using the new column **text normalized** from blogs dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blogs['text normalized'] = df_blogs.Review.apply(lambda x: pre_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_blogs['text normalized'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting name of coffee shops and adding to stop-words. This procedure is manual because we can find names as baked, coffee, tea, pastry, for instances. We don't know which names must be filtered until visualize this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Beacon Coffee & Pantry', 'Contraband Coffeebar', 'The Station SF',\n",
       "       'Mazarine Coffee', 'Verve Coffee Roasters',\n",
       "       'Ritual Coffee Roasters', 'Philz Coffee', 'Mercury Cafe',\n",
       "       'Jane on Fillmore', 'Saint Frank', 'Nook', 'Réveille Coffee Co.',\n",
       "       'Duboce Park Cafe', 'Atlas Cafe', 'The Grove',\n",
       "       'The Interval at Long Now', '20th Century Cafe',\n",
       "       'Sightglass Coffee', 'Verve', 'The\\xa0Mill SF', 'Cafe Réveille',\n",
       "       'Trouble Coffee', 'Blue Bottle Coffee', 'Four Barrel Coffee',\n",
       "       'Mission Heirloom', 'Paramo Coffee', 'Le Marais Bakery', 'Hollow',\n",
       "       'Red Door Coffee', 'Andytown Coffee Roasters', 'Garden House Cafe',\n",
       "       'Snowbird Coffee', 'Flywheel Coffee', 'fifty/fifty',\n",
       "       'Ritual Roasters Coffee', 'The Mill',\n",
       "       'Wrecking Ball Coffee Roasters', 'Lady Falcon Coffee Club',\n",
       "       'Linea Caffe', 'George and Lennie', 'Coffee Cultures',\n",
       "       'Sextant Coffee Roasters', 'Tartine Manufactory',\n",
       "       'Equator Coffees & Teas', 'Caffe Trieste',\n",
       "       'Chapel Hill Coffee Co.', 'La Capra Coffee', 'Provender',\n",
       "       \"Farley's\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blogs['Names'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Vectorizer function to **docs**\n",
    "\n",
    "Make a dataFrame with the resultant vocabulary, **df_key_groups**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = key_word_extractor(docs)[0]\n",
    "key_values = key_word_extractor(docs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words = []\n",
    "l_values = []\n",
    "for word in key_words:\n",
    "    l_words.append(word)\n",
    "    \n",
    "for value in key_values:\n",
    "    l_values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keys = pd.DataFrame({'key_words': l_words, 'key_values': l_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_words</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>espresso drink</th>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold brew</th>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another location</th>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authentic soda</th>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastry free</th>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  key_values\n",
       "key_words                   \n",
       "espresso drink         0.115\n",
       "cold brew              0.120\n",
       "another location       0.130\n",
       "authentic soda         0.130\n",
       "pastry free            0.130"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keys_group = df_keys.groupby('key_words').mean().sort_values('key_values')\n",
    "df_keys_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading customers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"../Data_Extraction/Reviews/reviews_rating_date.csv\", \\\n",
    "                         usecols=['Coffee', 'Description','Rating', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting proportion of Rating, we reduce the positive reviews, selecting 500 randomly with 5 stars and 500 with 4 stars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coffee</th>\n",
       "      <th>Description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0 star rating</th>\n",
       "      <td>245</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0 star rating</th>\n",
       "      <td>196</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0 star rating</th>\n",
       "      <td>305</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0 star rating</th>\n",
       "      <td>766</td>\n",
       "      <td>821</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0 star rating</th>\n",
       "      <td>1756</td>\n",
       "      <td>1930</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Coffee  Description  date\n",
       "Rating                                    \n",
       "1.0 star rating     245          263   263\n",
       "2.0 star rating     196          207   207\n",
       "3.0 star rating     305          327   327\n",
       "4.0 star rating     766          821   821\n",
       "5.0 star rating    1756         1930  1930"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.groupby('Rating').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling data with more reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_ = df_reviews[df_reviews['Rating'] == '1.0 star rating']\n",
    "df2_ = df_reviews[df_reviews['Rating'] == '2.0 star rating']\n",
    "df3_ = df_reviews[df_reviews['Rating'] == '3.0 star rating']\n",
    "df4_ = df_reviews[df_reviews['Rating'] == '4.0 star rating']\n",
    "df5_ = df_reviews[df_reviews['Rating'] == '5.0 star rating']\n",
    "\n",
    "df4_sample = df4_.sample(n=500, random_state=42)\n",
    "df5_sample = df5_.sample(n=500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data** is the new vector used on the analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df1_, df2_, df3_, df4_sample, df5_sample], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 5% of every category to add features into the **Coffee Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = data[data.Rating == '1.0 star rating'].sample(frac=0.05, random_state=42)\n",
    "n2 = data[data.Rating == '2.0 star rating'].sample(frac=0.05, random_state=42)\n",
    "n3 = data[data.Rating == '3.0 star rating'].sample(frac=0.05, random_state=42)\n",
    "n4 = data[data.Rating == '4.0 star rating'].sample(frac=0.05, random_state=42)\n",
    "n5 = data[data.Rating == '5.0 star rating'].sample(frac=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a dataFrame called **samples_dict**, we proceed to apply the preprocessing and the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dict = pd.concat([n1, n2, n3, n4, n5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dynamo Donut &', 'The Revolution', nan, 'Trouble Coffee',\n",
       "       'Saint Frank', 'Philz', 'Martha & Brothers', 'Golden Bear Trading',\n",
       "       'Beanstalk', 'Ritual Coffee', 'YakiniQ', 'Le Marais',\n",
       "       'Flywheel Coffee', 'U :Dessert', 'Sightglass', 'Vive La',\n",
       "       'Blue Bottle', 'Spike’s Coffees &', 'Tartine Bakery &',\n",
       "       'Equator Coffees &', 'Little', 'The', 'Black', 'Mazarine',\n",
       "       'Les Gourmands', 'Cantata Coffee', 'Contraband Coffee', 'Coffee',\n",
       "       'Workshop Cafe', 'Urban', 'Spin City', 'Paramo Coffee',\n",
       "       'UpForDayz', 'Java Beach', 'Réveille Coffee',\n",
       "       'Howard Street Coffee', 'Craftsman and', 'Morning Due',\n",
       "       'Weaver’s Coffee &', 'Socola Chocolatier and', 'Four Barrel',\n",
       "       'Henry’s House of', 'ilana’s', 'Wrecking Ball Coffee', 'Saltroot',\n",
       "       'Earth’s', 'Cafe', 'Alamo Square', 'Le Cafe du', 'Oakside',\n",
       "       'Central Coffee Tea &', 'Pinhole', 'Faye’s', 'Crostini &',\n",
       "       'Cinderella Bakery &'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_dict['Coffee'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dict['text normalized'] = samples_dict.Description.apply(lambda x: pre_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coffee</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>date</th>\n",
       "      <th>text normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Dynamo Donut &amp;</td>\n",
       "      <td>The donuts are good but the service is horribl...</td>\n",
       "      <td>1.0 star rating</td>\n",
       "      <td>2/23/2019</td>\n",
       "      <td>donut good service horrible run flavor early t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>The Revolution</td>\n",
       "      <td>the music is great but make sure you buy somet...</td>\n",
       "      <td>1.0 star rating</td>\n",
       "      <td>11/13/2018</td>\n",
       "      <td>music great sure buy something else rude guy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I used to work at this cafe and couldn't stand...</td>\n",
       "      <td>1.0 star rating</td>\n",
       "      <td>8/17/2017</td>\n",
       "      <td>used work not stand working longer week owner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>Trouble Coffee</td>\n",
       "      <td>Horrible staff. I just came in on the phone an...</td>\n",
       "      <td>1.0 star rating</td>\n",
       "      <td>2/25/2019</td>\n",
       "      <td>horrible staff came phone told gf wait ordered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>Saint Frank</td>\n",
       "      <td>This is the worst coffee I've tasted in SF. Th...</td>\n",
       "      <td>1.0 star rating</td>\n",
       "      <td>1/8/2019</td>\n",
       "      <td>worst tasted almond milk sucked drink almond m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Coffee                                        Description  \\\n",
       "1548  Dynamo Donut &  The donuts are good but the service is horribl...   \n",
       "1559  The Revolution  the music is great but make sure you buy somet...   \n",
       "3475             NaN  I used to work at this cafe and couldn't stand...   \n",
       "2836  Trouble Coffee  Horrible staff. I just came in on the phone an...   \n",
       "946      Saint Frank  This is the worst coffee I've tasted in SF. Th...   \n",
       "\n",
       "               Rating        date  \\\n",
       "1548  1.0 star rating   2/23/2019   \n",
       "1559  1.0 star rating  11/13/2018   \n",
       "3475  1.0 star rating   8/17/2017   \n",
       "2836  1.0 star rating   2/25/2019   \n",
       "946   1.0 star rating    1/8/2019   \n",
       "\n",
       "                                        text normalized  \n",
       "1548  donut good service horrible run flavor early t...  \n",
       "1559  music great sure buy something else rude guy s...  \n",
       "3475  used work not stand working longer week owner ...  \n",
       "2836  horrible staff came phone told gf wait ordered...  \n",
       "946   worst tasted almond milk sucked drink almond m...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_customer = samples_dict['text normalized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_key_words = key_word_extractor(docs_customer)[0]\n",
    "customer_key_values = key_word_extractor(docs_customer)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words = []\n",
    "l_values = []\n",
    "for word in customer_key_words:\n",
    "    l_words.append(word)\n",
    "    \n",
    "for value in customer_key_values:\n",
    "    l_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With exactly the same proceed, we build a second dataFrame with key-words from customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keys_customer = pd.DataFrame({'key_words': l_words, 'key_values': l_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group key words using the mean of the tfidf value, avoiding duplicates key-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_words</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>repeat customer</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opt sweetness</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sylvain chaillout</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sylvain looked</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sylvain very</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   key_values\n",
       "key_words                    \n",
       "repeat customer          0.07\n",
       "opt sweetness            0.07\n",
       "sylvain chaillout        0.07\n",
       "sylvain looked           0.07\n",
       "sylvain very             0.07"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keys_group_customer = df_keys_customer.groupby('key_words').mean().sort_values('key_values')\n",
    "df_keys_group_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we concatenate both (from blogs and customers) and save it as **tdidf_vocab**, available in the folder **preprocessing_ml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([df_keys_group_customer, df_keys_group]).to_csv('preprocessing_ml/tfidf_vocab.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
